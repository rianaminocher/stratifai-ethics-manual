[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STRATIF-AI: Framework for Ethical and Trustworthy Design",
    "section": "",
    "text": "About"
  },
  {
    "objectID": "index.html#why-do-we-need-a-framework",
    "href": "index.html#why-do-we-need-a-framework",
    "title": "STRATIF-AI: Framework for Ethical and Trustworthy Design",
    "section": "Why do we need a framework?",
    "text": "Why do we need a framework?\nAI policy tends to be high-level, so that it can apply to a wide variety of systems. Thus, guidelines need to be translated to project-specific requirements.\nMoreover, the regulatory landscape is constantly changing, as new technology continues to emerge. Adhering to principles of ethics and trustworthiness is a responsibility of AI systems developers.\nSTRATIF-AI is a novel concept and technology. Existing guidelines may not be relevant, nor do they provide sufficiently detailed recommendations.\nThis framework will provide concrete, co-created solutions to (1) ensure STRATIF-AI will comply with present regulations and (2) anticipate future barriers to implementation and uptake."
  },
  {
    "objectID": "1_oversight.html#fundamental-rights",
    "href": "1_oversight.html#fundamental-rights",
    "title": "1  Human Agency and Oversight",
    "section": "1.2 Fundamental rights",
    "text": "1.2 Fundamental rights"
  },
  {
    "objectID": "1_oversight.html#human-agency",
    "href": "1_oversight.html#human-agency",
    "title": "1  Human Agency and Oversight",
    "section": "1.2 Human agency",
    "text": "1.2 Human agency"
  },
  {
    "objectID": "1_oversight.html#human-oversight",
    "href": "1_oversight.html#human-oversight",
    "title": "1  Human Agency and Oversight",
    "section": "1.2 Human oversight",
    "text": "1.2 Human oversight\nQuestions:\n\nPlease determine whether the AI system (choose as many as appropriate): Is a self-learning or autonomous system; Is overseen by a Human-in-the-Loop; Is overseen by a Human-on-the-Loop; Is overseen by a Human-in-Command.\nHave the humans (human-in-the-loop, human-on-the-loop, human-in-command) been given specific training on how to exercise oversight?\nDid you establish any detection and response mechanisms for undesirable adverse effects of the AI system for the end-user or subject?\nDid you ensure a ‘stop button’ or procedure to safely abort an operation when needed?\nDid you take any specific oversight and control measures to reflect the self-learning or autonomous nature of the AI system?\n\nSolutions:\n\nEvaluate and ensure physician trust in the system\n\nDescription: For the system to be useful, physicians should\nA comprehensive training program for doctors on STRATIF-AI usage and interpretation should be made available before application. The program should include: discussion of limitations, protocol for overriding recommendations, a simplified explanation of the AI (“blackboxed”) system, value placed on medical expertise.\nexperience (HAO-05-R05) Requirement: Physician mistrust system’s decision due to use"
  },
  {
    "objectID": "2_technical.html#resilience-to-attack-and-security",
    "href": "2_technical.html#resilience-to-attack-and-security",
    "title": "2  Technical Robustness and Safety",
    "section": "2.1 Resilience to attack and security",
    "text": "2.1 Resilience to attack and security\nQuestions:\n\nCould the AI system have adversarial, critical or damaging effects (e.g. to human or societal safety) in case of risks or threats such as design or technical faults, defects, outages, attacks, misuse, inappropriate or malicious use?\nIs the AI system certified for cybersecurity (e.g. the certification scheme created by the Cybersecurity Act in Europe) or is it compliant with specific security standards?\nHow exposed is the AI system to cyber-attacks?\nDid you assess potential forms of attacks to which the AI system could be vulnerable?\nDid you consider different types of vulnerabilities and potential entry points for attacks such as:\nData poisoning (i.e. manipulation of training data);\nModel evasion (i.e. classifying the data according to the attacker’s will);\nModel inversion (i.e. infer the model parameters)\nDid you put measures in place to ensure the integrity, robustness and overall security of the AI system against potential attacks over its lifecycle?\nDid you red-team/pentest the system?\nDid you inform end-users of the duration of security coverage and updates?\nWhat length is the expected timeframe within which you provide security updates for the AI system?\n\nSolutions:"
  },
  {
    "objectID": "2_technical.html#fallback-plan-and-general-safety",
    "href": "2_technical.html#fallback-plan-and-general-safety",
    "title": "2  Technical Robustness and Safety",
    "section": "2.2 Fallback plan and general safety",
    "text": "2.2 Fallback plan and general safety"
  },
  {
    "objectID": "2_technical.html#accuracy",
    "href": "2_technical.html#accuracy",
    "title": "2  Technical Robustness and Safety",
    "section": "2.3 Accuracy",
    "text": "2.3 Accuracy\nQuestions:\n\nCould a low level of accuracy of the AI system result in critical, adversarial or damaging consequences?\nDid you put in place measures to ensure that the data (including training data) used to develop the AI system is up-to-date, of high quality, complete and representative of the environment the system will be deployed in?\nDid you put in place a series of steps to monitor, and document the AI system’s accuracy?\nDid you consider whether the AI system’s operation can invalidate the data or assumptions it was trained on, and how this might lead to adversarial effects?\nDid you put processes in place to ensure that the level of accuracy of the AI system to be expected by end-users and/or subjects is properly communicated?\n\nSolutions:"
  },
  {
    "objectID": "2_technical.html#reliability-and-reproducibility",
    "href": "2_technical.html#reliability-and-reproducibility",
    "title": "2  Technical Robustness and Safety",
    "section": "2.4 Reliability and reproducibility",
    "text": "2.4 Reliability and reproducibility\nQuestions:\n\nCould the AI system cause critical, adversarial, or damaging consequences (e.g. pertaining to human safety) in case of low reliability and/or reproducibility?\nDid you put in place a well-defined process to monitor if the AI system is meeting the intended goals?\nDid you test whether specific contexts or conditions need to be taken into account to ensure reproducibility?\nDid you put in place verification and validation methods and documentation (e.g. logging) to evaluate and ensure different aspects of the AI system’s reliability and reproducibility?\nDid you clearly document and operationalise processes for the testing and verification of the reliability and reproducibility of the AI system?\nDid you define tested failsafe fallback plans to address AI system errors of whatever origin and put governance procedures in place to trigger them?\nDid you put in place a proper procedure for handling the cases where the AI system yields results with a low confidence score?\nIs your AI system using (online) continual learning?\nDid you consider potential negative consequences from the AI system learning novel or unusual methods to score well on its objective function?\n\nSolutions:"
  },
  {
    "objectID": "3_privacy.html#privacy-and-data-protection",
    "href": "3_privacy.html#privacy-and-data-protection",
    "title": "3  Privacy and Data Governance",
    "section": "3.1 Privacy and data protection",
    "text": "3.1 Privacy and data protection"
  },
  {
    "objectID": "3_privacy.html#quality-and-integrity-of-data",
    "href": "3_privacy.html#quality-and-integrity-of-data",
    "title": "3  Privacy and Data Governance",
    "section": "3.2 Quality and integrity of data",
    "text": "3.2 Quality and integrity of data"
  },
  {
    "objectID": "3_privacy.html#access-to-data",
    "href": "3_privacy.html#access-to-data",
    "title": "3  Privacy and Data Governance",
    "section": "3.3 Access to data",
    "text": "3.3 Access to data"
  },
  {
    "objectID": "4_transparency.html#traceability",
    "href": "4_transparency.html#traceability",
    "title": "4  Transparency",
    "section": "4.1 Traceability",
    "text": "4.1 Traceability\nQuestions:\n\nDid you put in place measures that address the traceability of the AI system during its entire lifecycle?\nDid you put in place measures to continuously assess the quality of the input data to the AI system?\nCan you trace back which data was used by the AI system to make a certain decision(s) or recommendation(s)?\nCan you trace back which AI model or rules led to the decision(s) or recommendation(s) of the AI system?\nDid you put in place measures to continuously assess the quality of the output(s) of the AI system?\nDid you put adequate logging practices in place to record the decision(s) or recommendation(s) of the AI system?"
  },
  {
    "objectID": "4_transparency.html#explainability",
    "href": "4_transparency.html#explainability",
    "title": "4  Transparency",
    "section": "4.2 Explainability",
    "text": "4.2 Explainability\nQuestions:\n\nDid you explain the decision(s) of the AI system to the users?29\nDo you continuously survey the users if they understand the decision(s) of the AI system?\n\nSolutions:"
  },
  {
    "objectID": "4_transparency.html#communication",
    "href": "4_transparency.html#communication",
    "title": "4  Transparency",
    "section": "4.3 Communication",
    "text": "4.3 Communication\nQuestions:\n\nIn cases of interactive AI systems (e.g., chatbots, robo-lawyers), do you communicate to users that they are interacting with an AI system instead of a human?\nDid you establish mechanisms to inform users about the purpose, criteria and limitations of the decision(s) generated by the AI system?\nDid you communicate the benefits of the AI system to users?\nDid you communicate the technical limitations and potential risks of the AI system to users, such as its level of accuracy and/ or error rates?\nDid you provide appropriate training material and disclaimers to users on how to adequately use the AI system?\n\nSolutions:"
  },
  {
    "objectID": "5_diversity.html#avoidance-of-unfair-bias",
    "href": "5_diversity.html#avoidance-of-unfair-bias",
    "title": "5  Diversity, non-discrimination and fairness",
    "section": "5.1 Avoidance of unfair bias",
    "text": "5.1 Avoidance of unfair bias\nQuestions:\n\nDid you establish a strategy or a set of procedures to avoid creating or reinforcing unfair bias in the AI system, both regarding the use of input data as well as for the algorithm design?\nDid you consider diversity and representativeness of end-users and/or subjects in the data?\nDid you test for specific target groups or problematic use cases?\nDid you research and use publicly available technical tools, that are state-of- the-art, to improve your understanding of the data, model and performance?\nDid you assess and put in place processes to test and monitor for potential biases during the entire lifecycle of the AI system (e.g. biases due to possible limitations stemming from the composition of the used data sets (lack of diversity, non-representativeness)?\nWhere relevant, did you consider diversity and representativeness of end-users and or subjects in the data?\nDid you put in place educational and awareness initiatives to help AI designers and AI developers be more aware of the possible bias they can inject in designing and developing the AI system?\nDid you ensure a mechanism that allows for the flagging of issues related to bias, discrimination or poor performance of the AI system?\nDid you establish clear steps and ways of communicating on how and to whom such issues can be raised?\nDid you identify the subjects that could potentially be (in)directly affected by the AI system, in addition to the (end-)users and/or subjects?\nIs your definition of fairness commonly used and implemented in any phase of the process of setting up the AI system?\nDid you consider other definitions of fairness before choosing this one?\nDid you consult with the impacted communities about the correct definition of fairness, i.e. representatives of elderly persons or persons with disabilities?\nDid you ensure a quantitative analysis or metrics to measure and test the applied definition of fairness?\nDid you establish mechanisms to ensure fairness in your AI system?\n\nSolutions:"
  },
  {
    "objectID": "5_diversity.html#accessibility-and-universal-design",
    "href": "5_diversity.html#accessibility-and-universal-design",
    "title": "5  Diversity, non-discrimination and fairness",
    "section": "5.2 Accessibility and universal design",
    "text": "5.2 Accessibility and universal design\nQuestions:\n\nDid you ensure that the AI system corresponds to the variety of preferences and abilities in society?\nDid you assess whether the AI system’s user interface is usable by those with special needs or disabilities or those at risk of exclusion?\nDid you ensure that information about, and the AI system’s user interface of, the AI system is accessible and usable also to users of assistive technologies (such as screen readers)?\nDid you involve or consult with end-users or subjects in need for assistive technology during the planning and development phase of the AI system?\nDid you ensure that Universal Design principles are taken into account during every step of the planning and development process, if applicable?\nDid you take the impact of the AI system on the potential end-users and/or subjects into account?\nDid you assess whether the team involved in building the AI system engaged with the possible target end-users and/or subjects?\nDid you assess whether there could be groups who might be disproportionately affected by the outcomes of the AI system?\nDid you assess the risk of the possible unfairness of the system onto the end-user’s or subject’s communities?\n\nSolutions:"
  },
  {
    "objectID": "5_diversity.html#human-agency",
    "href": "5_diversity.html#human-agency",
    "title": "5  Diversity, non-discrimination and fairness",
    "section": "5.3 Human agency",
    "text": "5.3 Human agency"
  },
  {
    "objectID": "5_diversity.html#stakeholder-participation",
    "href": "5_diversity.html#stakeholder-participation",
    "title": "5  Diversity, non-discrimination and fairness",
    "section": "5.3 Stakeholder participation",
    "text": "5.3 Stakeholder participation\nQuestions:\n\nDid you consider a mechanism to include the participation of the widest range of possible stakeholders in the AI system’s design and development?"
  },
  {
    "objectID": "6_societal.html#sustainable-and-environmentally-friendly-ai",
    "href": "6_societal.html#sustainable-and-environmentally-friendly-ai",
    "title": "6  Societal and Environmental Well-being",
    "section": "6.1 Sustainable and environmentally friendly AI",
    "text": "6.1 Sustainable and environmentally friendly AI\nQuestions:\n\nAre there potential negative impacts of the AI system on the environment? o Which potential impact(s) do you identify?\nWhere possible, did you establish mechanisms to evaluate the environmental impact of the AI system’s development, deployment and/or use (for example, the amount of energy used and carbon emissions)?\nDid you define measures to reduce the environmental impact of the AI system throughout its lifecycle?\n\nSolutions:"
  },
  {
    "objectID": "6_societal.html#social-impact",
    "href": "6_societal.html#social-impact",
    "title": "6  Societal and Environmental Well-being",
    "section": "6.2 Social impact",
    "text": "6.2 Social impact\nQuestions:\n\nDoes the AI system impact human work and work arrangements?\nDid you pave the way for the introduction of the AI system in your organisation by informing and consulting with impacted workers and their representatives (trade unions, (European) work councils) in advance?\nDid you adopt measures to ensure that the impacts of the AI system on human work are well understood?\nDid you ensure that workers understand how the AI system operates, which capabilities it has and which it does not have?\nCould the AI system create the risk of de-skilling of the workforce?\nDid you take measures to counteract de-skilling risks?\nDoes the system promote or require new (digital) skills?\nDid you provide training opportunities and materials for re- and up-skilling?\n\nSolutions:"
  },
  {
    "objectID": "6_societal.html#society-and-democracy",
    "href": "6_societal.html#society-and-democracy",
    "title": "6  Societal and Environmental Well-being",
    "section": "6.3 Society and democracy",
    "text": "6.3 Society and democracy\nQuestions:\n\nCould the AI system have a negative impact on society at large or democracy?\nDid you assess the societal impact of the AI system’s use beyond the (end-)user and subject, such as potentially indirectly affected stakeholders or society at large?\nDid you take action to minimize potential societal harm of the AI system? o Did you take measures that ensure that the AI system does not negatively impact democracy?\n\nSolutions:"
  },
  {
    "objectID": "7_accountability.html#minimization-and-reporting-of-negative-impacts",
    "href": "7_accountability.html#minimization-and-reporting-of-negative-impacts",
    "title": "7  Accountability",
    "section": "7.1 Minimization and reporting of negative impacts",
    "text": "7.1 Minimization and reporting of negative impacts"
  },
  {
    "objectID": "7_accountability.html#tradeoffs",
    "href": "7_accountability.html#tradeoffs",
    "title": "7  Accountability",
    "section": "7.2 Tradeoffs",
    "text": "7.2 Tradeoffs"
  },
  {
    "objectID": "7_accountability.html#redress",
    "href": "7_accountability.html#redress",
    "title": "7  Accountability",
    "section": "7.3 Redress",
    "text": "7.3 Redress"
  },
  {
    "objectID": "index.html#stratif-ai",
    "href": "index.html#stratif-ai",
    "title": "STRATIF-AI: Framework for Ethical and Trustworthy Design",
    "section": "STRATIF-AI",
    "text": "STRATIF-AI\nSTRATIF-AI is a novel concept and technology.\nThe tool, based on AI and real-time data, will simulate patient lifetimes and response to treatments, to help physicians make quicker diagnoses as well as motivate patients to follow rehabilitation schedules. Such data-driven simulations of real-world systems are known as “digital twins”, and are utilized widely in the manufacturing and design industries.\nWhile STRATIF-AI is being developed to treat, detect and rehabilitate patients of stroke, the successful development of the tool will serve as a blueprint for utilizing digital-twin technology in healthcare. This will contribute a pivotal step towards a future of personalized medicine."
  },
  {
    "objectID": "index.html#framework",
    "href": "index.html#framework",
    "title": "STRATIF-AI: Framework for Ethical and Trustworthy Design",
    "section": "Framework",
    "text": "Framework\nThe STRATIF-AI Ethical Framework has been designed in concordance with the EU Guidelines for Trustworthy AI. The manual will be updated following audits of the STRATIF-AI consortium’s adherance to the framework, and an external Z-inspection of the ethical solution.\nThe guidelines list seven high-level requirements:\nThe Assessment List for Trustworthy AI (ALTAI), developed by the HLEG in 2020, was the starting template for our manual. This is an checklist intended to self-evaluate a tool, based on feedback from a six month long piloting process within the European AI community.\n“This Assessment List for Trustworthy AI (ALTAI) is intended for flexible use: organisations can draw on elements relevant to the particular AI system from this Assessment List for Trustworthy AI (ALTAI) or add elements to it as they see fit, taking into consideration the sector they operate in.”\nThis Assessment List for Trustworthy AI (ALTAI) is best completed involving a multidisciplinary team of people. These could be from within and/or outside your organisation with specific competences or expertise on each of the 7 requirements and related questions. Among the stakeholders you may find for example the following: • AI designers and AI developers of the AI system; • data scientists; • procurement officers or specialists; • front-end staff that will use or work with the AI system; • legal/compliance officers; • management."
  },
  {
    "objectID": "index.html#ethical-and-trustworthy-design",
    "href": "index.html#ethical-and-trustworthy-design",
    "title": "STRATIF-AI: Framework for Ethical and Trustworthy Design",
    "section": "Ethical and trustworthy design",
    "text": "Ethical and trustworthy design\nAs medical AI proliferates, the trustworthiness of its technology has come under scrutiny. Tools fail to perform in new contexts or fail to conform to minimum reporting standards set out by governing bodies. Technology may be based on data which is not representative, which could perpetuate harmful biases and increase inequality.\nSTRATIF-AI involves numerous novel technological solutions. A hybrid machine-learning and mechanistic modelling framework will be utilized to simulate patient-specific responses to changes in behaviour, diet, exercise and medication. Models will be trained on diverse sources of electronic health data, collected from 6 clinical studies, within a federated learning platform. Data will be uploaded and stored in a Personal Data Vault—to ensure patient control—and subjected to a semantic harmonization process to make them interoperable. Such a multi-faceted technology is yet unprecedented and could indeed spur new ethical dilemmas.\nTo ensure the design of an ethical and trustworthy tool, a dedicated effort involving all relevant project stakeholders, and a wide range of expertise, is necessary. For STRATIF-AI, we utilized a multi-pronged approach to construct a framework for ethics and trustworthiness.\n\nTranslation\nExisting guidelines or soft-law documents tend to be rather high-level or generally applicable. This makes them appropriate for a diversity of AI-systems, but they lack specific guidance on implementation for individual AI systems. We utilize expertise from the “Responsible Algorithms” team at the Berlin Institute for Health at Charite to develop strategic and project-specific solutions to ensure STRATIF-AI can adhere to AI policy.\n\n\nCo-creation\nWe are committed to equitable development of solutions. The framework for ethical design is being co-created with relevant stakeholders within the STRATIF-AI project, i.e., clinicians, data scientists, medical researchers, patient organization representatives and project coordinators. We thus assess the risks and benefits of various requirements to ensure ethics and trustworthiness, to create necessary, feasible and auditable ethical requirements.\n\n\nAnticipation\nThe regulatory landscape in the EU is evolving rapidly. With the advent of new technology often comes new legislation. Many aspects of STRATIF-AI are novel and may thus be overlooked by current guidelines. Solutions based solely on existing regulations will be reactive rather than prescriptive, having to rely on the deployment and failure of technology to guide decision-making.\nTo pre-empt such changes, we are conducting a scoping review of the ethical considerations in the field of digital twin technology in health-care. We additionally play close attention to concerns that may lie outside of the framework, during workshops with key project personnel."
  },
  {
    "objectID": "index.html#stratif-ai-.",
    "href": "index.html#stratif-ai-.",
    "title": "STRATIF-AI: Framework for Ethical and Trustworthy Design",
    "section": "0.1 STRATIF-AI {.}",
    "text": "0.1 STRATIF-AI {.}\nThe STRATIF-AI platform proposes a novel concept and data-driven technology to support clinical decision-making and personalized healthcare. This involves the development of numerous novel technological solutions. A hybrid machine-learning and mechanistic modelling framework will be utilized to simulate patient-specific responses to changes in behaviour, diet, exercise and medication. Models will be trained on diverse sources of electronic health data, collected from 6 clinical studies, within a federated learning platform. Data will be uploaded and stored in a Personal Data Vault—to ensure patient control—and subjected to a semantic harmonization process to make them interoperable.\nThese data and models will enable real-time simulations of specific patient journeys, intended to guide physicians in making diagnoses and treatment plans, as well as motivate patients to follow rehabilitation schedules. Such data-driven simulations of real-world systems are known as ``digital twins’’, and are utilized widely in the manufacturing and design industries. At present, STRATIF-AI is being developed to treat, detect and rehabilitate patients of stroke. However, the successful design and deployment of the platform is intended to serve as a template for digital-twin based health technology, and will thus contribute a pivotal step towards a future of personalized medicine."
  },
  {
    "objectID": "index.html#framework-overview",
    "href": "index.html#framework-overview",
    "title": "STRATIF-AI: Framework for Ethical and Trustworthy Design",
    "section": "Framework overview",
    "text": "Framework overview\nThis manual has been designed in concordance with the EU Guidelines for Trustworthy AI. The manual will be updated following audits of the STRATIF-AI consortium’s adherence to the framework, and an external Z-inspection of the ethical solution.\nThe EU Guidelines for Trustworthy AI were developed on 8 April 2019 by the High-Level Expert Group (HLEG) on AI. The guidelines aim to promote three key principles to support the development and deployment of safe AI systems—i.e., lawfulness, ethics and robustness. The principles are expounded as seven key requirements.\nOur manual was first derived by following the self-assessment checklist—Assessment List for Trustworthy AI (ALTAI)—developed by the HLEG in 2020. The ALTAI checklist is intended to help organizations identify key elements and concepts to design ethical AI systems.\nThis framework will provide concrete, co-created solutions to (1) ensure STRATIF-AI will comply with present regulations and (2) anticipate future barriers to implementation and uptake."
  },
  {
    "objectID": "0_rights.html",
    "href": "0_rights.html",
    "title": "Fundamental Rights Impact Assessment",
    "section": "",
    "text": "The protection of people’s fundamental rights is the term in the European Union used to refer to the human rights set forth within the EU Treaties, the Charter of Fundamental Rights and Internal Human Rights Law. These rights encompass dignity and non-discrimination, data protection and privacy. As recommended by policy-makers, this manual begins with a broad assessment of fundamental rights. The relevant solutions are linked below in response to each assessment question.\n1. Does the AI system potentially negatively discriminate against people on the basis of any of the following grounds (non-exhaustively): sex, race, colour, ethnic or social origin, genetic features, language, religion or belief, political or any other opinion, membership of a national minority, property, birth, disability, age or sexual orientation?\n\nlist bias testing plans for development, deployment, and use\nlist processes to address bias\n\n2. Does the AI system respect the rights of the child, for example with respect to child protection and taking the child’s best interests into account?\n3. Does the AI system protect personal data relating to individuals in line with GDPR?\n\ndata protection impact assessment, including necessity/proportionality of processing operations in relation to their purpose\nrisks, safeguards, security measures for personal data\n\n4. Does the AI system respect the freedom of expression and information and/or freedom of assembly and association?\n\nwhich mechanisms are in place/testing/monitoring systems"
  },
  {
    "objectID": "1_oversight.html#human-agency-and-autonomy",
    "href": "1_oversight.html#human-agency-and-autonomy",
    "title": "1  Human Agency and Oversight",
    "section": "1.1 Human agency and autonomy",
    "text": "1.1 Human agency and autonomy\nQuestions:\n\nIs the AI system designed to interact, guide or take decisions by human end-users that affect humans or society?\nCould the AI system generate confusion for some or all end-users or subjects on whether a decision, content, advice or outcome is the result of an algorithmic decision?\nAre end-users or other subjects adequately made aware that a decision, content, advice or outcome is the result of an algorithmic decision?\nCould the AI system generate confusion for some or all end-users or subjects on whether they are interacting with a human or AI system?\nAre end-users or subjects informed that they are interacting with an AI system?\nDoes the AI system risk creating human attachment, stimulating addictive behaviour, or manipulating user behaviour? Depending on which risks are possible or likely, please answer the questions below:\nDid you take measures to deal with possible negative consequences for end-users or subjects in case they develop a disproportionate attachment to the AI System?\nDid you take measures to minimise the risk of addiction?\nDid you take measures to mitigate the risk of manipulation?\n\nSolutions:\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\nDescription\nKey Personnel\nIndicators\nProject Phase\nAction Items\nNotes\n\n\n\n\nProvide training on medical decision-making\nA comprehensive training program for doctors on STRATIF-AI usage and interpretation should be made available before application. The program should include: discussion of limitations, protocol for overriding recommendations, a simplified explanation of the AI (“blackboxed”) system, value placed on medical expertise.\nWP4 Prevention, WP5 Treatment, WP6 Rehabilitation\nWhat is the status of training program development? What percentage of doctors have received training? What proportion of decisions are overridden by doctors?\nDevelopment\nLeads to be appointed to draft training program\nIs this a part of any clinical study?\n\n\n\n\nCommunicate the role of AI in decision-making\n\nDescription: A standardized procedure for explaining the use of the tool and involvement of the tool to patients should be designed and adhered to.\nKey personnel: WP4 Prevention, WP5 Treatment, WP6 Rehabilitation—Leads to be appointed to draft training program.\nProgress: Has standardized documentation been drafted? Do visual materials exist? Have patient doubts/mistrust been assessed? Has patient understanding been verified?\nPhase: Clinical validation/Deployment\n\nPrioritize epistemic authority"
  },
  {
    "objectID": "2_technical.html#general-safety",
    "href": "2_technical.html#general-safety",
    "title": "2  Technical Robustness and Safety",
    "section": "2.2 General safety",
    "text": "2.2 General safety\nQuestions:\n\nDid you define risks, risk metrics and risk levels of the AI system in each specific use case?\nDid you put in place a process to continuously measure and assess risks?\nDid you inform end-users and subjects of existing or potential risks?\nDid you identify the possible threats to the AI system (design faults, technical faults, environmental threats) and the possible consequences?\nDid you assess the risk of possible malicious use, misuse or inappropriate use of the AI system?\nDid you define safety criticality levels (e.g. related to human integrity) of the possible consequences of faults or misuse of the AI system?\nDid you assess the dependency of a critical AI system’s decisions on its stable and reliable behaviour?\nDid you align the reliability/testing requirements to the appropriate levels of stability and reliability?\nDid you plan fault tolerance via, e.g. a duplicated system or another parallel system (AI-based or ‘conventional’)?\nDid you develop a mechanism to evaluate when the AI system has been changed to merit a new review of its technical robustness and safety?\n\nSolutions:"
  },
  {
    "objectID": "3_privacy.html#privacy",
    "href": "3_privacy.html#privacy",
    "title": "3  Privacy and Data Governance",
    "section": "3.1 Privacy",
    "text": "3.1 Privacy\nQuestions:\n\nDid you consider the impact of the AI system on the right to privacy, the right to physical, mental and/or moral integrity and the right to data protection?\nDepending on the use case, did you establish mechanisms that allow flagging issues related to privacy concerning the AI system?\n\nSolutions:"
  },
  {
    "objectID": "3_privacy.html#data-governance",
    "href": "3_privacy.html#data-governance",
    "title": "3  Privacy and Data Governance",
    "section": "3.2 Data governance",
    "text": "3.2 Data governance\nQuestions:\n\nIs your AI system being trained, or was it developed, by using or processing personal data (including special categories of personal data)?\nDid you put in place any of the following measures some of which are mandatory under the General Data Protection Regulation (GDPR), or a non-European equivalent?\nData Protection Impact Assessment (DPIA)\nDesignate a Data Protection Officer (DPO) and include them at an early state in the development, procurement or use phase of the AI system\nOversight mechanisms for data processing (including limiting access to qualified personnel, mechanisms for logging data access and making modifications);\nMeasures to achieve privacy-by-design and default (e.g. encryption, pseudonymisation, aggregation, anonymisation)\nData minimisation, in particular personal data (including special categories of data);\nDid you implement the right to withdraw consent, the right to object and the right to be forgotten into the development of the AI system?\nDid you consider the privacy and data protection implications of data collected, generated or processed over the course of the AI system’s life cycle?\nDid you consider the privacy and data protection implications of the AI system’s non-personal training-data or other processed non-personal data?\nDid you align the AI system with relevant standards (e.g. ISO25, IEEE26) or widely adopted protocols for (daily) data management and governance?\n\nSolutions:\n100% of datasets fulfill quality criteria:To be defined with regards to (Missing data, errors, inaccuracies, interoperability) before final model training for demonstrator."
  },
  {
    "objectID": "7_accountability.html#auditability",
    "href": "7_accountability.html#auditability",
    "title": "7  Accountability",
    "section": "7.1 Auditability",
    "text": "7.1 Auditability\n\nDid you establish mechanisms that facilitate the AI system’s auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system’s processes, outcomes, positive and negative impact)?\nDid you ensure that the AI system can be audited by independent third parties?"
  },
  {
    "objectID": "7_accountability.html#tradeoffs-risk-management",
    "href": "7_accountability.html#tradeoffs-risk-management",
    "title": "7  Accountability",
    "section": "7.2 Tradeoffs/ Risk-management",
    "text": "7.2 Tradeoffs/ Risk-management\n\nDid you foresee any kind of external guidance or third-party auditing processes to oversee ethical concerns and accountability measures?\nDoes the involvement of these third parties go beyond the development phase?\nDid you organise risk training and, if so, does this also inform about the potential legal framework applicable to the AI system?\nDid you consider establishing an AI ethics review board or a similar mechanism to discuss the overall accountability and ethics practices, including potential unclear grey areas?\nDid you establish a process to discuss and continuously monitor and assess the AI system’s adherence to this Assessment List for Trustworthy AI (ALTAI)?\nDoes this process include identification and documentation of conflicts between the 6 aforementioned requirements or between different ethical principles and explanation of the ‘trade-off’ decisions made?\nDid you provide appropriate training to those involved in such a process and does this also cover the legal framework applicable to the AI system?\nDid you establish a process for third parties (e.g. suppliers, end-users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?\nDoes this process foster revision of the risk management process?\nFor applications that can adversely affect individuals, have redress by design mechanisms been put in place?"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "STRATIF-AI: Framework for Ethical and Trustworthy Design",
    "section": "",
    "text": "STRATIF-AI is a novel concept and technology.\nThe tool, based on AI and real-time data, will simulate patient lifetimes and response to treatments, to help physicians make quicker diagnoses as well as motivate patients to follow rehabilitation schedules. Such data-driven simulations of real-world systems are known as “digital twins”, and are utilized widely in the manufacturing and design industries.\nWhile STRATIF-AI is being developed to treat, detect and rehabilitate patients of stroke, the successful development of the tool will serve as a blueprint for utilizing digital-twin technology in healthcare. This will contribute a pivotal step towards a future of personalized medicine."
  },
  {
    "objectID": "index.html#socio-technical-scenarios-from-the-z-inspection-process",
    "href": "index.html#socio-technical-scenarios-from-the-z-inspection-process",
    "title": "STRATIF-AI: Framework for Ethical and Trustworthy Design",
    "section": "Socio-technical scenarios from the Z-inspection process",
    "text": "Socio-technical scenarios from the Z-inspection process"
  },
  {
    "objectID": "index.html#socio-technical-scenarios",
    "href": "index.html#socio-technical-scenarios",
    "title": "STRATIF-AI: Framework for Ethical and Trustworthy Design",
    "section": "Socio-technical scenarios",
    "text": "Socio-technical scenarios"
  }
]