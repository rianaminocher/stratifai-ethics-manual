# Accountability

The principle of accountability highlights the need to establish mechanisms to guarantee responsibility throughout the lifecycle of AI systems. This principle intersects closely with risk management practices, which involve transparently identifying and mitigating risks that can be comprehended and audited by external parties. In instances where unjust or adverse impacts arise, accessible mechanisms for accountability must be established, to facilitate the opportunity for redress to affected parties.

## Auditability

### ALTAI Assessment {.unnumbered}

The auditability of STRATIF-AI's workflow and system lifecycle is a fundamental aspect of our ethical plan. By creating a living document, we aim to establish a transparent and traceable log of all measures taken to adhere to ethical principles and legal requirements. To ensure the AI system can be audited by independent third parties, the ethical plan will be made available or disseminated accordingly. 

### Facilitate auditability

**Description** ***Methodology*** to facilitate external audits of STRATIF-AI processes will be instituted and ***validated***.

***Methodology:*** A publicly available and version-controlled ethical plan; *TBD*  
***validated:*** validated by Z-inspection process; validated within Delphi study    

**Owner**  

* WP3

**Key Personnel**

* WP7
* WP2

**Schedule**  At any point, before end of project.  

**Stroke Phase** ALL

**Actionable tasks**  

* appoint key personnel
* define additional outputs?
* discuss site prototype and any legal barriers
* solicit feedback on prototype
* create sub-page to report important parameters
* define important parameters
* e.g. traceability of the development process, the sourcing of training data and the logging of the AI systemâ€™s processes, outcomes, positive and negative impact
* make pages public

### Adhere to ethical/open research norms

**Description** Research work produced by the STRATIF-AI consortium must adhere to EU Horizon funding guidelines as well as ethical norms regarding the ***data availability***, ***reproducibility***, ***open access***, ***distribution***, ***confidentiality*** and ***originality*** of published work.

***data availability:*** data made available in Zenodo repository; data adheres to FAIR principles; data is labelled as clearly associated research publication   
***reproducibility:*** scientific code is made available on Github; scientific code is made available on Zenodo  
***open access:*** research published under "gold" open access standards; research published in journal with CC-BY license   
***distribution:*** authorship roles are allocated within CRediT taxonomy; fraudulent authorship is prevented ("guest", "gift", "ghost"); authors meet minimum authorship criteria;   
***confidentiality:*** sensitive information about patients is protected; permission from primary author must be obtained before dissemination;  
***originality:*** use of any AI-assisted technology is disclosed; reference to published/unoriginal work is appropriately cited;  

::: {.callout-note}
Further information on the CRediT taxonomy for assigning authorship is available online at
https://credit.niso.org. The Royal Society Publishing Editorial team also provides tips on publishing ethics here https://royalsociety.org/blog/2022/03/authorship-contributions-disputes-misconduct/. 
:::

**Owner**  

* WP3

**Key Personnel**

* all WP leads

**Schedule**  At any point, before end of project.  

**Stroke Phase** ALL

**Actionable tasks**  

* appoint key personnel
* create publication guide, to be signed before publication by authors
* implement SOP for publication within consortium

## Tradeoffs/ Risk-management

### ALTAI Assessment {.unnumbered}

It is essential to identify, assess, document, and mitigate potential negative impacts of AI systems, particularly for those directly or indirectly affected. Safeguards must be in place to protect whistleblowers, NGOs, trade unions, and other entities reporting legitimate concerns about AI systems. As part of WP3 within the STRATIF-AI project, we will conduct an external Z-inspection assessment, which is a gold-standard and validated methodology to assess the ethical impacts of an AI tool. A comprehensive risk training protocol will be designed and adhere to any legal protocols. Our ethical plan has been designed in concordance with the Assessment List for Trustworthy AI (ALTAI), and, as such, will be continually evaluated throughout the AI system's lifecycle. 

While implementing these requirements, tensions may arise, necessitating inevitable trade-offs. We will identify relevant interests and values implicated by the AI system and explicitly evaluate trade-offs in terms of their risk to safety and ethical principles, including fundamental rights. These decisions will be thoroughly documented throughout our ethical audit process. Additionally, a  a process for third parties (e.g. suppliers, end-users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system, and a following risk management protocol will be instituted. Accessible mechanisms to ensure adequate redress will be established.

### Institute external reporting system

**Description** A system for ***third-parties*** to make reports about ***risks*** should be established within the platform and ***periodically*** be evaluated.

***third-parties:*** citizens; hospital workers; developers; *any contributors of technology or data*   
***risks:***  vulnerabilities; errors; concerns   
***periodically:*** *TBD*   

**Stroke Phase** ALL

**Owner**  

* WP2

**Key Personnel**

* design staff
* technical staff
* WP3

**Actionable tasks**

* appoint key personnel
* define all parameters
* define link to feedback reporting loop
* implement/design procedure
* feedback from third parties to validate
* test system

::: {.callout-note}
In relation to requirement 6.3.1, it will be necessary to solicit and encourage feedback from non-users and potential users to mitigate risks of misuse or under-user of the STRATIF-AI platform.
:::

### Collect post-prediction feedback

**Description** A system for ***end-users*** to provide feedback about ***medical applicability*** should be established within the platform. 

::: {.callout-note}
This is closely related to the requirement 1.2.2 [Human Agency and Oversight](1_oversight.qmd); Institute reporting feedback loop.
:::

***end-users:*** medical professionals  
***medical applicability:*** effectiveness; relevance; alignment with evolving medical knowledge and practices; 

**Stroke Phase** EACH (3)

**Owner**  

* WP4
* WP5
* WP6

**Key Personnel**

* medical staff
* design staff
* technical staff

**Actionable tasks**

* appoint key personnel
* medical professionals to define key parameters
* prototype for system/information parameters
* implementation by WP2/ design staff
* design and validate system

### Conduct budget impact analysis

**Description** The economic justification of the implementation of the STRATIF-AI platform for different ***stakeholders*** must be made via cost-benefit analyses.

***stakeholders:*** hospitals; insurance companies; patients; rehab centers; *any affected institutional parties?*

**Stroke Phase** EACH (3)

**Owner**  

* WP7

**Key Personnel**

* medical staff

**Actionable tasks**

* appoint key personnel
* define parameters 
* design budget impact/ cost-benefit analysis study
* conduct analysis
* report results
* publication/dissemination of results
